# -*- coding: utf-8 -*-
"""Resume Selector with Naive Bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1evi_36bdRFRGsl0r-dr11sUJEVEUZY9R

# TASK #1: UNDERSTAND THE PROBLEM AND BUSINESS CASE

![image.png](attachment:image.png)
"""

# Data Source: https://www.kaggle.com/samdeeplearning/deepnlp

"""![image.png](attachment:image.png)

# TASK #2: IMPORT LIBRARIES AND DATASETS
"""

# install nltk
!pip install nltk

# install gensim
!pip install gensim

!pip install wordcloud

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from sklearn.metrics import classification_report, confusion_matrix

# load the data
resume_df = pd.read_csv('/content/resume.csv',encoding='latin-1')

resume_df.head()

# data containing resume
resume_df.drop('resume_id',axis=1,inplace=True)

resume_df.head()

"""MINI CHALLENGE #1: 
- Print the first and last elements in the dataframe. 
"""

resume_df.head(10)

resume_df.tail(10)

"""# TASK #3: PERFORM EXPLORATORY DATA ANALYSIS"""

# obtain dataframe information
resume_df.info()

# check for null values
resume_df.isnull().sum()

resume_df['class'].value_counts()

"""MINI CHALLENGE #2:
- Divide the DataFrame into two, one that belongs to class 0 and 1. Do we have a balanced dataset?
"""

resume_df['class'] = resume_df['class'].apply(lambda x: 0 if x == 'not_flagged' else 1)

resume_df_flagged = resume_df[resume_df['class'] == 1]
resume_df_notflagged = resume_df[resume_df['class'] == 0]
if len(resume_df_flagged)!=len(resume_df_notflagged):
  print('Not balanced!')
else:
  print('balanced')

resume_df_flagged

resume_df_notflagged

"""# TASK #4: PERFORM DATA CLEANING"""

resume_df['resume_text'] = resume_df['resume_text'].apply(lambda x: x.replace('\r',''))

resume_df

# download nltk packages
nltk.download('punkt')

# download nltk packages
nltk.download("stopwords")

# Get additional stopwords from nltk
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from','subject','re','edu','com','use','email'])

# Remove stop words and remove words with 2 or less characters
def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text) :
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2 and token not in stop_words:
            result.append(token)
            
    return ' '.join(result)

# Cleaned text
resume_df['cleaned'] = resume_df['resume_text'].apply(preprocess)

resume_df

print(resume_df['cleaned'][0])

print(resume_df['resume_text'][0])



"""# TASK #5: VISUALIZE CLEANED DATASET"""

# Plot the counts of flagged vs not flagged

sns.countplot(x='class',data = resume_df)

# plot the word cloud for text that is not flagged
plt.figure(figsize = (20,20)) 
wc = WordCloud(max_words = 2000, height = 800,width = 1600, stopwords=stop_words).generate(str(resume_df[resume_df['class']==0].cleaned))
plt.imshow(wc)

"""MINI CHALLENGE #3:
- Plot the wordcloud for class #1 
"""

# plot the word cloud for text that is flagged
plt.figure(figsize = (20,20)) 
wc = WordCloud(max_words = 2000, height = 800,width = 1600, stopwords=stop_words).generate(str(resume_df[resume_df['class']==1].cleaned))
plt.imshow(wc)

"""# TASK #6: PREPARE THE DATA BY APPLYING COUNT VECTORIZER

![image.png](attachment:image.png)
"""

# CountVectorizer example
from sklearn.feature_extraction.text import CountVectorizer
sample_data = ['This is the first document.','This document is the second document.','And this is the third one.','Is this the first document?']
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(sample_data)

print(vectorizer.get_feature_names_out())

print(X.toarray())

# Applying CountVectorier to the cleaned text
vectorizer = CountVectorizer()
countvectorizer = vectorizer.fit_transform(resume_df['cleaned'])

print(vectorizer.get_feature_names_out())

print(countvectorizer.toarray())

"""# TASK #7: UNDERSTAND THE THEORY AND INTUITION BEHIND NAIVE BAYES CLASSIFIERS - PART #1

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

# TASK #8: UNDERSTAND THE THEORY AND INTUITION BEHIND NAIVE BAYES CLASSIFIERS - PART #2

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

MINI CHALLENGE #4:
- Calculate the probability of the red class (non-retiring).
"""

# P(non_retire/X) = P(X/non_retire)*P(non_retire)/P(X)

# P(non_retire/X) = ((3/20) * (20/60))/(4/60) = 3/4 = 0.75

"""# TASK#9: TRAIN NAIVE BAYES CLASSIFIER MODEL"""

X = countvectorizer
y = resume_df['class']

X.shape

y.shape

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)

from sklearn.naive_bayes import MultinomialNB

NB_classifier = MultinomialNB()
NB_classifier.fit(X_train,y_train)



"""# TASK #10: ASSESS TRAINED MODEL PERFORMANCE

![image.png](attachment:image.png)
"""

# Predicting the performance on train data
y_predict_train = NB_classifier.predict(X_train)
cm = confusion_matrix(y_train, y_predict_train)
sns.heatmap(cm, annot = True)

# Predicting the Test set results
y_predict_test = NB_classifier.predict(X_test)
cm = confusion_matrix(y_test, y_predict_test)
sns.heatmap(cm, annot = True)

# classification report
print(classification_report(y_test, y_predict_test))

"""MINI CHALLENGE #5:
- Retrain the model after spliting the data into 10% testing and 90% training and assess model performance

"""

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1)

NB_classifier = MultinomialNB()
NB_classifier.fit(X_train,y_train)

y_predict_test = NB_classifier.predict(X_test)
cm = confusion_matrix(y_test, y_predict_test)
sns.heatmap(cm, annot = True)

print(classification_report(y_test, y_predict_test))

"""![image.png](attachment:image.png)"""

